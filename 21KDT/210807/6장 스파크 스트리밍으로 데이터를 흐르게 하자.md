# 6장

> ## 스파크 스트리밍으로 데이터를 흐르게 하자

**학습 내용**

- 이상 스트림을 사용하는 방법
- 시간에 따라 변환하는 계산 상태를 저장하는 방법
- 윈도 연산을 사용하는 방법
- 아파치 카프카와 데이터를 주고받는 방법
- 최적의 스트리밍 처리 성능을 달성하는 방법



스파크는 강력한 대규모 데이터 분석 기능 외에도 동일한 API로 스트리밍 프로그램과 일괄 처리 프로그램을 모두 지원하는 통합 플랫폼을 제공한다.

다시 말해 실시간 레이어와 배치 레이어를 결합한 람다 아키텍처를 구축할 수 있다.



스파크 스트리밍은 

- 하둡과 호환되는 여러 **파일 시스템**과 
  - 예 : DHFS, 아마존 S3
- 다양한 **분산 시스템**에서 
  - 예 : 플럼, 카프카(대표적 범용 툴), 트위터 등

데이터를 읽어 들일 수 있는 커넥터를 제공





OLTP

배치 처리성 작업



## 6.1 스파크 스트리밍 애플리케이션 작성

람다 아키텍처를 바탕으로 강력한 대규모 스트림 분석 기능

- streaming processing + batch processing

미니배치 : 특정 시간 간격 내에 유입된 데이터 블록을 RDD로 구성한다

아래 그림은 미니배치 개념을 도식화 한것

![스파크를 다루는 기술: 6.1 스파크 스트리밍 애플리케이션 작성](https://thebook.io/img/006908/spark229.jpg)

- 아파치 스타크의 스트리밍 데이터 처리 방식(스파크 스트리밍은 입력 데이터 스트림을 미니배치 RDD로 시분할 한다. 다른 스파크 컴포넌트는 이 미니배치 RDD를 일반 RDD처럼 처리한다.)

233 ~ 238



### 6.1.7 시간에 따라 변화하는 계산 상태 저장

![스파크를 다루는 기술: 6.1.7 시간에 따라 변화하는 계산 상태 저장 - 1](https://thebook.io/img/006908/spark239.jpg)

- 스파크 스트리밍이 시간에 따라 변하는 상태를 유지하는 방법 (DStream은 현재 미니배치로 입수된 새로운 데이터와 마지막 상태에 저장된 과거 데이터를 결합하고, 결과를 계산한 후 새로운 데이터로 상태를 갱신한다)



## 6.2 외부 데이터 소스 사용



## 6.3 스파크 스트리밍의 잡 성능



## 6.4 정형 스트리밍



## 6.5 요약



스칼라, 스파크로 시작하기 

3,**4**,5,**6**장 실습한거 a4용지 2장 정도 
