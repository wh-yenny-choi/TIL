# 결정 트리 Ver01

## 🧠리뷰

정확도

불균형한 데이터 셋 

- 작은 영향력이 드러나지 않아 분류 시 

  정밀도, 재현율과 수치를 같이 비교해야함

  제대로 맞은 건수만 판단

재현율 관점

정밀도 관점

- 상호보완적 
- 적정선 찾기

predic_proba()함수

임곗값 - 기준치 조절

예측 << 분류

데이터 대부분 범주형 => 분류

분류에 좋은 성능을 보여주는 알고리즘

- 이진분류, 사진분류....
- 기본적 개념 이해



## 분류(Classification)

**지도학습**

- 레이블(정답)이 있는 데이터가 주어진 상태에서 학습하는 머신러닝 방식
- 레이블이 수치로 주어짐 ⇒ 회귀
- 레이블이 범주로 주어짐 ⇒ 분류



**분류(Classificaion)**

- 지도학습의 대표적 유형
- 학습 데이터로 주어진 데이터의 피처와 레이블값(결정값, 클래스값)을 머신러닝 알고리즘으로 학습해 모델을 생성하고 생성된 모델에 새로운 값이 주어졌을 때 미지의 레이블 값을 예측하는 것
- 기존 데이터가 어떤 레이블에 속하는지 알고리즘을 통해 패턴을 인지한 뒤 새롭데 관측된 데이터에 대한 레이블을 판별



### 대표적인 분류 알고리즘

- 결정 트리(Decision Tree) : 데이터 균일도(불순도)에 따른 규칙 기반
- 나이브 베이즈(Navie Bayes) :  베이즈 통계와 생성 모델의 기반
  - 확률 기반으로 사전-사후 확률 분류 
- 로지스틱 회귀(Logistic Regression) : 독립변수와 종속변수의 선형 관계성
- 서포트 벡터 머신(Support Vector Machine) :  개별 클래스 간의 최대 분류 마진을 효과적으로 찾음
  - 비선형 형태 (선형X) 이용해 분류 
- 최소 근접 알고리즘(Nearest Neighbor) : 근접 거리 기준
- 신경망(Neural Network) : 심층 연결 기반
- 앙상블(Ensemble) : 여러 머신러닝 알고리즘 결합
  - 서로 다른(또는 같은) 머신러닝 알고리즘을 결합한 앙상블



#### 앙상블 방법(Ensemble Method)

분류에서 가장 각광 밥는 방법 중 하나

이미지, 영상, 음성, NLP영역에서 신경망에 기반한 딥러닝이 머신러닝계를 선도하고 있지만, 이를 제외한 정형 데이터의 예측 분석 영역에서는 앙상블이 **매우 높은 예측 성능**으로 인해 많은 분석가와 데이터 과학자들에게 애용되고 있음

서로 다른/또는 같은 알고리즘을 단순히 결합한 형태도 있으나, 일반적으로

- 배깅 (Bagging) 
- 부스팅 (Boosting)
- 스태킹 (Stacking)

##### 배깅 방식

대표적으로 랜덤 포레스트(Random Forest) 알고리즘

- 뛰어난 예측 성능
- 상대적으로 빠른 수행 시간
- 유연성 등

##### 부스팅 방식

그래디언트 부스팅 (Gradient Boosting)

- 장점 : 뛰어난 예측 성능
- 단점 : 너무 오래 걸리는 수행 시간
  - 최적화 모델 튜닝이 어려움

기존 그래디언트 부스팅의 예측 성능을 발전시키면서도 수행 시간을 단축시킨 알고리즘

- XgBoost (eXtra Gradient Boost)
- LightGBM

앙상블은 서로 다른/또는 같은 알고리즘을 결합한다고 했는데, 대부분은 동일한 알고리즘을 결합한다. 앙상블의 기본 알고리즘으로 일반적으로 사용하는 것은 **결정트리**

- 매우 쉽고 유연하게 적용될 수 있는 알고리즘
- 데이터의 스케일링이나 정규화 등의 사전 가공의 영향이 매우 적음
- 하지만 예측성능을 향상시키기 위해 복잡한 규칙 구조를 가짐
  - 과적합(overfitting) 발생 ⇒ 예측 성능 저하의 단점이 있다
    - 이런 단점은 앙상블 기법에서는 오히려 장점으로 작용

앙상블은 매우 많은 여러 개의 약한 학습기를 결합해 확률적 보완과 오류가 발생한 부분에 대한 가중치를 계속 업데이트하면서 예측 성능을 향상시키는데, 결정 트리가 좋은 약한 학습기임

- 약한 학습기 : 예측 성능이 상대적으로 떨어지는 학습 알고리즘



## 결정 트리(Decision Tree)

ML알고리즘 중 직관적으로 이해하기 쉬운 알고리즘

분류와 회귀  문제에서 가장 널리 사용

- 분류 + 회귀 분석까지도 가능

학습을 통해 데이터에 있는 규칙을 학습을 통해 자동으로 찾아내 트리(Tree) 기반의 분류 규칙을 만드는 것

특정 기준(질문)에 따라서 데이터를 구분

일반적으로 가장 쉬운 표현 방법 : if/else 기반 (스무고개 게임과 유사)

- 룰 기반의 프로그램에 적용되는 if, else를 자동으로 찾아내 예측을 위한 규칙을 만드는 알고리즘

따라서 데이터의 어떤 기준을 바탕으로 규칙을 만들어야 가장 효율적인 분류가 될 것인가가 알고리즘의 성능을 크게 좌우한다.



### 의사결정나무의 타입

분류 나무

- 범주형 목표 변수를 기준으로 마디를 나눔
- 끝마디에 포함된 자료의 범주가 분류 결과 값이 됨

회귀 나무

- 연속형 목표 변수를 기준으로 마디를 나눔
- 끝마디에 포함된 자료의 평균값이 각 끝마디의 회귀 값이 됨



### Tree 구조

전체적인 모양이 나무를 뒤집어 놓은 것과 닮았다고 해서 붙여진 이름

결정 트리에서 질문이나 네모상자를 노드(Node)라고 함

맨 위의 노드(첫 질문)을 Root Node

각 질문에 대한 마지막 노드를 Leaf Node : 결정된 클래스 값

Decision Nod(규칙 노드) : 규칙 조건

새로운 규칙 조건마다 Sub Tree 생성

![image-20210728092159183](머신러닝09 결정 트리.assets/image-20210728092159183.png)

- Depth = 3
- 이진 트리 형태

노드 

분류된 결과 - 분홍

루트 노드 - 초록

상대적 하위에 있는 노드 - 자식노드 / 부모노드

최하위 노드 - 단말노드, 더이상 나눠지지 않음 - 분홍 

나누어지는 노드 = 자식노드

자신노드의 상위 노드 = 부모노드

하위에 속하는 노드 = sub-tree



### 결정트리에서 중요한 이슈

데이터 세트의 피처가 결합해 규칙 조건을 만들 때마다 규칙노드가 만들어짐

**규칙이 많아지면** 결정 방식이 복잡해지고 **과적합(overfitting)** 발생

- 즉 **depth가 깊어질수록** 결정트리의 예측 성능이 저하될 가능성이 높음

가능한 적은 결정노드로 높은 예측 정확도를 가지려면

- 데이터를 분류할 때 최대한 많은 데이터 세트가 해당 분류에 속할 수 있도록 결정 노드의 규칙이 정해져야 함

- **어떻게 트리를 분할할 것인가**가 중요
  - 최대한 균일한 데이터 세트를 구성할 수 있도록 분할하는 것이 필요
  - 순도가 높은것 = 불순도를 낮추는 것
  - 불순도는 낮추는 관점에서 데이터를 분할



### 가지치기 (purning)

특정 노드에서 계속 분할 되다가, 예측 성능이 좋아지지 않는 부분이 오면 가지치기 (예측 정확도를 높이는데 효과가 없을 때) , 일반화 성능

- 특정 노드 밑의 하부 트리를 제거하여 일반화 성능을 높이는 방법
- 깊이가 줄어들고 결과의 개수가 줄어듦
- 트리에 가지가 너무 많으면 과적합(overfitting)
- 과적합(overfitting)을 막기 위한 방법



### 결정트리의 장단점

#### 장점

- 매우 쉽고 유연하게 적용될 수 있는 알고리즘
- 정보 균일도 룰을 기반으로 알고리즘이 쉽고 직관적으로 명확함
- 데이터 스케일링이나 정규화 등의 전처리 영향이 적음

#### 단점

- 규칙이 많아지면 결정 방식이 복잡해지고 과적합(overfit)발생
- 즉, depth가 길어질수록 결정트리의 예측 성능이 저하될 가능성이 높음



### 결정트리 알고리즘 성능

**데이터의 어떤 기준을 바탕으로**규칙을 만들어야 **가장 효율적인 분류**가 될 것인가가 **알고리즘의 성능**을 크게 좌우 

가능한 적은 결정노드로 옾은 예측 정확도를 가지려면



![image-20210728093258996](머신러닝09 결정 트리.assets/image-20210728093258996.png)





### 균일도

![image-20210728093441562](머신러닝09 결정 트리.assets/image-20210728093441562.png)

균일도를 높이는 것

불순도는 낮추는 것



#### 엔트로피

- 데이터 분

#### 정보 이득 지수

- 1 - 엔트로피 지수
  - 얼마나 순도가 높아졌는지

#### 지니 계수

- 불평등도
  - 평등 = 0
- 결정 트리 에서는 최대치 0.5

#### 결정 트리 알고리즘에서 지니 계수 이용

지니 계수를 기반으로 데이터 셋 분류

- 기본적으로 지니 계수를 이용해서 데이터 세트 분할
- 데이터 세트를 분할하거나 가장 좋은 조건
  - 정보 이득이 높거나 지니 계수가 낮은 조건

![image-20210728093833886](머신러닝09 결정 트리.assets/image-20210728093833886.png)

- p값에 따라 그래프 변화
- 평균에 대해서 말하는 것 => 분산을 최소화 , 데이터 분할
- 분산 : 데이터 평균으로부터의 거리 개념
  - 얼마나 퍼져 있는지?
  - 같은 데이터값끼리 있으면 분산은 거의 0에 가까움
  - 분산이 작아지는 방향으로 회귀 분류에서는 사용된다
  - 분산을 분할의 지표로 사용
- 불순도 측정 수식으로 이해
- 0에 가까울 수록 순도에 가까움 = 같은 클래스의 데이터끼리 모여 있다



### 결정 트리 알고리즘에서 분류를 결정하는 과정

![image-20210728094451973](머신러닝09 결정 트리.assets/image-20210728094451973.png)



### 사이킷런의 결정트리 알고리즘 클래스

- ㅇ

![image-20210728103522070](머신러닝09 결정 트리.assets/image-20210728103522070.png)

![image-20210728103701010](머신러닝09 결정 트리.assets/image-20210728103701010.png)



### 💻실습

#### 결정 트리 모델의 시각화

#### 붓꽃 데이터 세트에 결정 트리 적용 및 시각화

DecisionTreeClassifier 이용해 학습한 뒤 규칙 트리 시각화

##### export_graphviz()함수

sklearn.tree 모듈은 Graphviz를 이용하기 위한 export_graphviz()함수 제공

```sql
from sklearn.tree import export_graphviz

export_graphviz(dt_clf, 
                out_file = 'tree.dot', 
                class_names = iris_data.target_names, 
                feature_names = iris_data.feature_names, 
                impurity = True, 
                filled = True)
```

매개변수
- 학습이 완료된 estimator 
    - dt_clf = DecisionTreeClassifier
- output 파일명
    - out_file = 'tree.dot'
    - export_graphviz()의 호출 결과로 out_file로 지정된 tree.dot파일을 생성함
- 결정 클래스 명칭
    - class_names = iris_data.target_names
- 피처 이름
    - feature_names = iris_data.feature_names
- impurity = True 
    - 각 노드에 불순도 표시 (Gini계수, 디폴트 : True) 
- filled = True 
    - 노드 색상 표시 (디폴트 : False)

**생성된 dot파일 출력 방법 2가지**
1. Graphviz 시각화 툴 사용 : .dot 파일 읽어서 출력
2. 이미지 파일로 변환해서 저장 후 출력



##### 실행 결과

![decision-tree](머신러닝09 결정 트리.assets/decision-tree.png)





<img src="머신러닝09 결정 트리.assets/image-20210728113843743.png" alt="image-20210728113843743" style="zoom:25%;" />





### 결정 트리 과적합

![image-20210728133607000](머신러닝09 결정 트리.assets/image-20210728133607000.png)



![image-20210728134552560](머신러닝09 결정 트리.assets/image-20210728134552560.png)



### 결정 트리 실습

![image-20210728134515757](머신러닝09 결정 트리.assets/image-20210728134515757.png)





![image-20210728152449901](머신러닝09 결정 트리.assets/image-20210728152449901.png)





## 🎈참조 Decision Tree

![image-20210728094602735](머신러닝09 결정 트리.assets/image-20210728094602735.png)

초록 / 파랑 / 빨강 데이터 분류

더 불순도를 낮출 수 있는 기준 선택 (X축 점선 or Y축 점선 = 기준)

![image-20210728094643413](머신러닝09 결정 트리.assets/image-20210728094643413.png)

Y값 기준으로 나누어짐 X값 기준으로

분류 규칙 - 균일도 계산

![image-20210728094858939](머신러닝09 결정 트리.assets/image-20210728094858939.png)

ㅇ

![image-20210728094938159](머신러닝09 결정 트리.assets/image-20210728094938159.png)

![image-20210728095024410](머신러닝09 결정 트리.assets/image-20210728095024410.png)

균일도 = 반대 순도

![image-20210728095039041](머신러닝09 결정 트리.assets/image-20210728095039041.png)

- 불순도가 0에 가까울수록 좋음
  - 왼쪽이 오른쪽보다 좋음

![image-20210728100702082](머신러닝09 결정 트리.assets/image-20210728100702082.png)

불순도 = 0.4278

![image-20210728101517383](머신러닝09 결정 트리.assets/image-20210728101517383.png)

- 같은 순도의 엔트로피 = 0

나아진 정도 

![image-20210728101342363](머신러닝09 결정 트리.assets/image-20210728101342363.png)

가지치기

![image-20210728101605274](머신러닝09 결정 트리.assets/image-20210728101605274.png)

예시

![image-20210728101618751](머신러닝09 결정 트리.assets/image-20210728101618751.png)

![image-20210728101742308](머신러닝09 결정 트리.assets/image-20210728101742308.png)

![image-20210728101828847](머신러닝09 결정 트리.assets/image-20210728101828847.png)

지니계수 이용 예시

![image-20210728101908049](머신러닝09 결정 트리.assets/image-20210728101908049.png)

- 0 = 2개 (2/5)
- 1= 3개 (3/5)

- 지니 계수 = 0.48

- X1, X2 모두 범주형 데이터

성별에 따라 분리 (X1)

![image-20210728102118135](머신러닝09 결정 트리.assets/image-20210728102118135.png)

학력에 따라 분리 (X2)

![image-20210728102155180](머신러닝09 결정 트리.assets/image-20210728102155180.png)

정리

- 학력 기준이 훨씬 큼 (약 0.2)

![image-20210728102254053](머신러닝09 결정 트리.assets/image-20210728102254053.png)

과적합

![image-20210728102358548](머신러닝09 결정 트리.assets/image-20210728102358548.png)

![image-20210728102449200](머신러닝09 결정 트리.assets/image-20210728102449200.png)

2번이 더 데이터 적합

- 1번이 과소
- 2번 너무 흔들리는거 과대

![image-20210728102534219](머신러닝09 결정 트리.assets/image-20210728102534219.png)

정지규칙 3가지 꼭 기억 

![image-20210728102859852](머신러닝09 결정 트리.assets/image-20210728102859852.png)

수직 방면으로 분류 경계면

![image-20210728102927313](머신러닝09 결정 트리.assets/image-20210728102927313.png)

데이터가 2차 

mse : 회귀에서의 지표 

- 원래 값 - 예측 값 차이, 잔차
- mse 줄이는것 = 좋은것 = 예측을 잘하는 것

![image-20210728103001254](머신러닝09 결정 트리.assets/image-20210728103001254.png)



